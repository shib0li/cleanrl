{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f76381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=43386, episodic_return=[62.]\n",
      "SPS: 8382\n",
      "global_step=43499, episodic_return=[113.]\n",
      "SPS: 8383\n",
      "global_step=43513, episodic_return=[14.]\n",
      "global_step=43600, episodic_return=[87.]\n",
      "SPS: 8387\n",
      "global_step=43618, episodic_return=[18.]\n",
      "global_step=43635, episodic_return=[17.]\n",
      "global_step=43660, episodic_return=[25.]\n",
      "global_step=43676, episodic_return=[16.]\n",
      "SPS: 8390\n",
      "global_step=43773, episodic_return=[97.]\n",
      "global_step=43789, episodic_return=[16.]\n",
      "SPS: 8394\n",
      "global_step=43805, episodic_return=[16.]\n",
      "global_step=43819, episodic_return=[14.]\n",
      "global_step=43833, episodic_return=[14.]\n",
      "global_step=43856, episodic_return=[23.]\n",
      "global_step=43870, episodic_return=[14.]\n",
      "SPS: 8394\n",
      "global_step=43911, episodic_return=[41.]\n",
      "global_step=43925, episodic_return=[14.]\n",
      "global_step=43952, episodic_return=[27.]\n",
      "global_step=43968, episodic_return=[16.]\n",
      "global_step=43990, episodic_return=[22.]\n",
      "SPS: 8396\n",
      "global_step=44015, episodic_return=[25.]\n",
      "global_step=44036, episodic_return=[21.]\n",
      "SPS: 8400\n",
      "global_step=44103, episodic_return=[67.]\n",
      "global_step=44139, episodic_return=[36.]\n",
      "global_step=44157, episodic_return=[18.]\n",
      "global_step=44198, episodic_return=[41.]\n",
      "SPS: 8404\n",
      "global_step=44209, episodic_return=[11.]\n",
      "global_step=44236, episodic_return=[27.]\n",
      "global_step=44267, episodic_return=[31.]\n",
      "SPS: 8408\n",
      "global_step=44305, episodic_return=[38.]\n",
      "global_step=44354, episodic_return=[49.]\n",
      "global_step=44368, episodic_return=[14.]\n",
      "SPS: 8412\n",
      "global_step=44470, episodic_return=[102.]\n",
      "SPS: 8416\n",
      "global_step=44524, episodic_return=[54.]\n",
      "SPS: 8419\n",
      "global_step=44625, episodic_return=[101.]\n",
      "global_step=44653, episodic_return=[28.]\n",
      "global_step=44671, episodic_return=[18.]\n",
      "global_step=44686, episodic_return=[15.]\n",
      "SPS: 8420\n",
      "global_step=44731, episodic_return=[45.]\n",
      "global_step=44747, episodic_return=[16.]\n",
      "global_step=44777, episodic_return=[30.]\n",
      "global_step=44791, episodic_return=[14.]\n",
      "SPS: 8421\n",
      "global_step=44813, episodic_return=[22.]\n",
      "global_step=44827, episodic_return=[14.]\n",
      "global_step=44862, episodic_return=[35.]\n",
      "global_step=44891, episodic_return=[29.]\n",
      "SPS: 8425\n",
      "global_step=44909, episodic_return=[18.]\n",
      "global_step=44938, episodic_return=[29.]\n",
      "global_step=44994, episodic_return=[56.]\n",
      "SPS: 8429\n",
      "global_step=45006, episodic_return=[12.]\n",
      "global_step=45091, episodic_return=[85.]\n",
      "SPS: 8431\n",
      "global_step=45141, episodic_return=[50.]\n",
      "global_step=45178, episodic_return=[37.]\n",
      "global_step=45200, episodic_return=[22.]\n",
      "SPS: 8435\n",
      "global_step=45216, episodic_return=[16.]\n",
      "global_step=45227, episodic_return=[11.]\n",
      "global_step=45253, episodic_return=[26.]\n",
      "global_step=45268, episodic_return=[15.]\n",
      "global_step=45300, episodic_return=[32.]\n",
      "SPS: 8438\n",
      "global_step=45347, episodic_return=[47.]\n",
      "global_step=45378, episodic_return=[31.]\n",
      "SPS: 8442\n",
      "global_step=45401, episodic_return=[23.]\n",
      "global_step=45446, episodic_return=[45.]\n",
      "global_step=45483, episodic_return=[37.]\n",
      "SPS: 8444\n",
      "global_step=45504, episodic_return=[21.]\n",
      "global_step=45523, episodic_return=[19.]\n",
      "global_step=45558, episodic_return=[35.]\n",
      "global_step=45581, episodic_return=[23.]\n",
      "SPS: 8447\n",
      "global_step=45609, episodic_return=[28.]\n",
      "global_step=45632, episodic_return=[23.]\n",
      "global_step=45655, episodic_return=[23.]\n",
      "global_step=45671, episodic_return=[16.]\n",
      "global_step=45699, episodic_return=[28.]\n",
      "SPS: 8449\n",
      "global_step=45751, episodic_return=[52.]\n",
      "SPS: 8453\n",
      "global_step=45803, episodic_return=[52.]\n",
      "global_step=45821, episodic_return=[18.]\n",
      "global_step=45872, episodic_return=[51.]\n",
      "global_step=45888, episodic_return=[16.]\n",
      "SPS: 8456\n",
      "global_step=45987, episodic_return=[99.]\n",
      "SPS: 8461\n",
      "global_step=46033, episodic_return=[46.]\n",
      "global_step=46063, episodic_return=[30.]\n",
      "global_step=46078, episodic_return=[15.]\n",
      "global_step=46099, episodic_return=[21.]\n",
      "SPS: 8464\n",
      "global_step=46136, episodic_return=[37.]\n",
      "global_step=46161, episodic_return=[25.]\n",
      "SPS: 8467\n",
      "global_step=46205, episodic_return=[44.]\n",
      "global_step=46231, episodic_return=[26.]\n",
      "global_step=46261, episodic_return=[30.]\n",
      "SPS: 8471\n",
      "global_step=46325, episodic_return=[64.]\n",
      "global_step=46348, episodic_return=[23.]\n",
      "SPS: 8476\n",
      "global_step=46413, episodic_return=[65.]\n",
      "global_step=46455, episodic_return=[42.]\n",
      "global_step=46481, episodic_return=[26.]\n",
      "SPS: 8480\n",
      "global_step=46516, episodic_return=[35.]\n",
      "global_step=46545, episodic_return=[29.]\n",
      "global_step=46574, episodic_return=[29.]\n",
      "SPS: 8484\n",
      "global_step=46611, episodic_return=[37.]\n",
      "global_step=46625, episodic_return=[14.]\n",
      "global_step=46646, episodic_return=[21.]\n",
      "global_step=46685, episodic_return=[39.]\n",
      "SPS: 8487\n",
      "global_step=46710, episodic_return=[25.]\n",
      "global_step=46728, episodic_return=[18.]\n",
      "global_step=46745, episodic_return=[17.]\n",
      "global_step=46773, episodic_return=[28.]\n",
      "SPS: 8492\n",
      "global_step=46825, episodic_return=[52.]\n",
      "SPS: 8490\n",
      "global_step=46913, episodic_return=[88.]\n",
      "global_step=46928, episodic_return=[15.]\n",
      "global_step=46961, episodic_return=[33.]\n",
      "SPS: 8494\n",
      "global_step=47016, episodic_return=[55.]\n",
      "global_step=47060, episodic_return=[44.]\n",
      "global_step=47080, episodic_return=[20.]\n",
      "SPS: 8498\n",
      "global_step=47108, episodic_return=[28.]\n",
      "global_step=47151, episodic_return=[43.]\n",
      "global_step=47188, episodic_return=[37.]\n",
      "SPS: 8497\n",
      "global_step=47218, episodic_return=[30.]\n",
      "global_step=47247, episodic_return=[29.]\n",
      "global_step=47273, episodic_return=[26.]\n",
      "global_step=47300, episodic_return=[27.]\n",
      "SPS: 8498\n",
      "global_step=47326, episodic_return=[26.]\n",
      "global_step=47361, episodic_return=[35.]\n",
      "global_step=47378, episodic_return=[17.]\n",
      "global_step=47390, episodic_return=[12.]\n",
      "SPS: 8501\n",
      "global_step=47441, episodic_return=[51.]\n",
      "SPS: 8504\n",
      "global_step=47531, episodic_return=[90.]\n",
      "global_step=47553, episodic_return=[22.]\n",
      "global_step=47578, episodic_return=[25.]\n",
      "SPS: 8505\n",
      "global_step=47603, episodic_return=[25.]\n",
      "global_step=47645, episodic_return=[42.]\n",
      "global_step=47662, episodic_return=[17.]\n",
      "global_step=47686, episodic_return=[24.]\n",
      "SPS: 8507\n",
      "global_step=47705, episodic_return=[19.]\n",
      "global_step=47721, episodic_return=[16.]\n",
      "global_step=47745, episodic_return=[24.]\n",
      "global_step=47765, episodic_return=[20.]\n",
      "global_step=47794, episodic_return=[29.]\n",
      "SPS: 8508\n",
      "global_step=47805, episodic_return=[11.]\n",
      "global_step=47843, episodic_return=[38.]\n",
      "global_step=47862, episodic_return=[19.]\n",
      "SPS: 8512\n",
      "global_step=47927, episodic_return=[65.]\n",
      "global_step=47945, episodic_return=[18.]\n",
      "global_step=47985, episodic_return=[40.]\n",
      "SPS: 8514\n",
      "global_step=48016, episodic_return=[31.]\n",
      "global_step=48032, episodic_return=[16.]\n",
      "global_step=48055, episodic_return=[23.]\n",
      "global_step=48091, episodic_return=[36.]\n",
      "SPS: 8514\n",
      "global_step=48104, episodic_return=[13.]\n",
      "global_step=48122, episodic_return=[18.]\n",
      "global_step=48163, episodic_return=[41.]\n",
      "SPS: 8516\n",
      "global_step=48247, episodic_return=[84.]\n",
      "global_step=48267, episodic_return=[20.]\n",
      "global_step=48294, episodic_return=[27.]\n",
      "SPS: 8520\n",
      "global_step=48317, episodic_return=[23.]\n",
      "global_step=48331, episodic_return=[14.]\n",
      "global_step=48357, episodic_return=[26.]\n",
      "global_step=48377, episodic_return=[20.]\n",
      "global_step=48389, episodic_return=[12.]\n",
      "SPS: 8520\n",
      "global_step=48411, episodic_return=[22.]\n",
      "global_step=48433, episodic_return=[22.]\n",
      "global_step=48474, episodic_return=[41.]\n",
      "SPS: 8521\n",
      "global_step=48507, episodic_return=[33.]\n",
      "global_step=48556, episodic_return=[49.]\n",
      "global_step=48572, episodic_return=[16.]\n",
      "global_step=48595, episodic_return=[23.]\n",
      "SPS: 8523\n",
      "global_step=48615, episodic_return=[20.]\n",
      "global_step=48633, episodic_return=[18.]\n",
      "global_step=48668, episodic_return=[35.]\n",
      "global_step=48693, episodic_return=[25.]\n",
      "SPS: 8527\n",
      "global_step=48703, episodic_return=[10.]\n",
      "global_step=48739, episodic_return=[36.]\n",
      "global_step=48787, episodic_return=[48.]\n",
      "SPS: 8530\n",
      "global_step=48813, episodic_return=[26.]\n",
      "SPS: 8535\n",
      "global_step=48938, episodic_return=[125.]\n",
      "global_step=48965, episodic_return=[27.]\n",
      "SPS: 8540\n",
      "global_step=49078, episodic_return=[113.]\n",
      "global_step=49092, episodic_return=[14.]\n",
      "SPS: 8536\n",
      "global_step=49104, episodic_return=[12.]\n",
      "global_step=49122, episodic_return=[18.]\n",
      "global_step=49136, episodic_return=[14.]\n",
      "global_step=49153, episodic_return=[17.]\n",
      "global_step=49169, episodic_return=[16.]\n",
      "global_step=49188, episodic_return=[19.]\n",
      "SPS: 8527\n",
      "global_step=49237, episodic_return=[49.]\n",
      "global_step=49282, episodic_return=[45.]\n",
      "global_step=49297, episodic_return=[15.]\n",
      "SPS: 8529\n",
      "global_step=49308, episodic_return=[11.]\n",
      "global_step=49331, episodic_return=[23.]\n",
      "global_step=49343, episodic_return=[12.]\n",
      "global_step=49371, episodic_return=[28.]\n",
      "global_step=49397, episodic_return=[26.]\n",
      "SPS: 8530\n",
      "global_step=49416, episodic_return=[19.]\n",
      "global_step=49429, episodic_return=[13.]\n",
      "global_step=49465, episodic_return=[36.]\n",
      "global_step=49495, episodic_return=[30.]\n",
      "SPS: 8534\n",
      "global_step=49510, episodic_return=[15.]\n",
      "global_step=49530, episodic_return=[20.]\n",
      "global_step=49588, episodic_return=[58.]\n",
      "SPS: 8531\n",
      "global_step=49610, episodic_return=[22.]\n",
      "global_step=49635, episodic_return=[25.]\n",
      "global_step=49684, episodic_return=[49.]\n",
      "SPS: 8531\n",
      "global_step=49725, episodic_return=[41.]\n",
      "global_step=49747, episodic_return=[22.]\n",
      "global_step=49769, episodic_return=[22.]\n",
      "global_step=49788, episodic_return=[19.]\n",
      "SPS: 8532\n",
      "global_step=49811, episodic_return=[23.]\n",
      "global_step=49861, episodic_return=[50.]\n",
      "global_step=49880, episodic_return=[19.]\n",
      "SPS: 8534\n",
      "global_step=49947, episodic_return=[67.]\n",
      "SPS: 8537\n",
      "global_step=50009, episodic_return=[62.]\n",
      "global_step=50031, episodic_return=[22.]\n",
      "global_step=50050, episodic_return=[19.]\n",
      "SPS: 8537\n",
      "global_step=50115, episodic_return=[65.]\n",
      "global_step=50135, episodic_return=[20.]\n",
      "global_step=50193, episodic_return=[58.]\n",
      "SPS: 8538\n",
      "global_step=50208, episodic_return=[15.]\n",
      "global_step=50299, episodic_return=[91.]\n",
      "SPS: 8540\n",
      "global_step=50322, episodic_return=[23.]\n",
      "global_step=50357, episodic_return=[35.]\n",
      "global_step=50367, episodic_return=[10.]\n",
      "global_step=50383, episodic_return=[16.]\n",
      "SPS: 8540\n",
      "global_step=50408, episodic_return=[25.]\n",
      "global_step=50435, episodic_return=[27.]\n",
      "global_step=50452, episodic_return=[17.]\n",
      "global_step=50475, episodic_return=[23.]\n",
      "global_step=50500, episodic_return=[25.]\n",
      "SPS: 8541\n",
      "global_step=50512, episodic_return=[12.]\n",
      "global_step=50552, episodic_return=[40.]\n",
      "global_step=50595, episodic_return=[43.]\n",
      "SPS: 8544\n",
      "global_step=50630, episodic_return=[35.]\n",
      "global_step=50672, episodic_return=[42.]\n",
      "SPS: 8546\n",
      "global_step=50711, episodic_return=[39.]\n",
      "global_step=50728, episodic_return=[17.]\n",
      "global_step=50790, episodic_return=[62.]\n",
      "SPS: 8548\n",
      "global_step=50805, episodic_return=[15.]\n",
      "global_step=50872, episodic_return=[67.]\n",
      "SPS: 8551\n",
      "global_step=50919, episodic_return=[47.]\n",
      "global_step=50952, episodic_return=[33.]\n",
      "global_step=50979, episodic_return=[27.]\n",
      "global_step=50997, episodic_return=[18.]\n",
      "SPS: 8552\n",
      "global_step=51017, episodic_return=[20.]\n",
      "global_step=51055, episodic_return=[38.]\n",
      "global_step=51074, episodic_return=[19.]\n",
      "SPS: 8555\n",
      "global_step=51108, episodic_return=[34.]\n",
      "global_step=51124, episodic_return=[16.]\n",
      "global_step=51154, episodic_return=[30.]\n",
      "global_step=51194, episodic_return=[40.]\n",
      "SPS: 8558\n",
      "global_step=51231, episodic_return=[37.]\n",
      "global_step=51249, episodic_return=[18.]\n",
      "global_step=51288, episodic_return=[39.]\n",
      "SPS: 8558\n",
      "global_step=51338, episodic_return=[50.]\n",
      "SPS: 8563\n",
      "global_step=51401, episodic_return=[63.]\n",
      "SPS: 8566\n",
      "global_step=51511, episodic_return=[110.]\n",
      "global_step=51536, episodic_return=[25.]\n",
      "global_step=51565, episodic_return=[29.]\n",
      "global_step=51592, episodic_return=[27.]\n",
      "SPS: 8567\n",
      "global_step=51630, episodic_return=[38.]\n",
      "SPS: 8572\n",
      "global_step=51702, episodic_return=[72.]\n",
      "global_step=51749, episodic_return=[47.]\n",
      "global_step=51776, episodic_return=[27.]\n",
      "SPS: 8574\n",
      "global_step=51832, episodic_return=[56.]\n",
      "SPS: 8577\n",
      "global_step=51922, episodic_return=[90.]\n",
      "global_step=51985, episodic_return=[63.]\n",
      "SPS: 8580\n",
      "global_step=52002, episodic_return=[17.]\n",
      "global_step=52071, episodic_return=[69.]\n",
      "global_step=52088, episodic_return=[17.]\n",
      "SPS: 8579\n",
      "global_step=52181, episodic_return=[93.]\n",
      "SPS: 8581\n",
      "global_step=52219, episodic_return=[38.]\n",
      "global_step=52230, episodic_return=[11.]\n",
      "global_step=52283, episodic_return=[53.]\n",
      "SPS: 8584\n",
      "global_step=52312, episodic_return=[29.]\n",
      "global_step=52357, episodic_return=[45.]\n",
      "global_step=52373, episodic_return=[16.]\n",
      "global_step=52389, episodic_return=[16.]\n",
      "SPS: 8587\n",
      "global_step=52425, episodic_return=[36.]\n",
      "global_step=52477, episodic_return=[52.]\n",
      "SPS: 8590\n",
      "global_step=52515, episodic_return=[38.]\n",
      "global_step=52554, episodic_return=[39.]\n",
      "global_step=52590, episodic_return=[36.]\n",
      "SPS: 8591\n",
      "global_step=52615, episodic_return=[25.]\n",
      "global_step=52634, episodic_return=[19.]\n",
      "global_step=52671, episodic_return=[37.]\n",
      "global_step=52690, episodic_return=[19.]\n",
      "SPS: 8594\n",
      "global_step=52702, episodic_return=[12.]\n",
      "global_step=52748, episodic_return=[46.]\n",
      "SPS: 8596\n",
      "global_step=52820, episodic_return=[72.]\n",
      "global_step=52879, episodic_return=[59.]\n",
      "SPS: 8598\n",
      "global_step=52909, episodic_return=[30.]\n",
      "global_step=52935, episodic_return=[26.]\n",
      "global_step=52950, episodic_return=[15.]\n",
      "global_step=52965, episodic_return=[15.]\n",
      "global_step=52997, episodic_return=[32.]\n",
      "SPS: 8600\n",
      "global_step=53053, episodic_return=[56.]\n",
      "SPS: 8602\n",
      "global_step=53158, episodic_return=[105.]\n",
      "global_step=53185, episodic_return=[27.]\n",
      "SPS: 8603\n",
      "global_step=53210, episodic_return=[25.]\n",
      "global_step=53245, episodic_return=[35.]\n",
      "global_step=53280, episodic_return=[35.]\n",
      "SPS: 8606\n",
      "global_step=53350, episodic_return=[70.]\n",
      "SPS: 8610\n",
      "global_step=53456, episodic_return=[106.]\n",
      "global_step=53494, episodic_return=[38.]\n",
      "SPS: 8614\n",
      "global_step=53521, episodic_return=[27.]\n",
      "global_step=53557, episodic_return=[36.]\n",
      "SPS: 8615\n",
      "global_step=53601, episodic_return=[44.]\n",
      "global_step=53642, episodic_return=[41.]\n",
      "global_step=53667, episodic_return=[25.]\n",
      "global_step=53685, episodic_return=[18.]\n",
      "SPS: 8618\n",
      "global_step=53723, episodic_return=[38.]\n",
      "global_step=53770, episodic_return=[47.]\n",
      "SPS: 8621\n",
      "global_step=53817, episodic_return=[47.]\n",
      "global_step=53847, episodic_return=[30.]\n",
      "SPS: 8625\n",
      "global_step=53925, episodic_return=[78.]\n",
      "global_step=53951, episodic_return=[26.]\n",
      "global_step=53997, episodic_return=[46.]\n",
      "SPS: 8626\n",
      "global_step=54023, episodic_return=[26.]\n",
      "global_step=54076, episodic_return=[53.]\n",
      "global_step=54092, episodic_return=[16.]\n",
      "SPS: 8628\n",
      "global_step=54134, episodic_return=[42.]\n",
      "global_step=54166, episodic_return=[32.]\n",
      "global_step=54197, episodic_return=[31.]\n",
      "SPS: 8630\n",
      "global_step=54257, episodic_return=[60.]\n",
      "SPS: 8632\n",
      "global_step=54350, episodic_return=[93.]\n",
      "global_step=54385, episodic_return=[35.]\n",
      "SPS: 8636\n",
      "global_step=54404, episodic_return=[19.]\n",
      "global_step=54430, episodic_return=[26.]\n",
      "global_step=54443, episodic_return=[13.]\n",
      "global_step=54485, episodic_return=[42.]\n",
      "SPS: 8638\n",
      "global_step=54502, episodic_return=[17.]\n",
      "global_step=54549, episodic_return=[47.]\n",
      "global_step=54567, episodic_return=[18.]\n",
      "global_step=54580, episodic_return=[13.]\n",
      "SPS: 8637\n",
      "global_step=54610, episodic_return=[30.]\n",
      "Moviepy - Building video /Users/shibo/Workspace/cleanrl/videos/CartPole-v1__dqn__1__1772033784/rl-video-episode-2000.mp4.\n",
      "Moviepy - Writing video /Users/shibo/Workspace/cleanrl/videos/CartPole-v1__dqn__1__1772033784/rl-video-episode-2000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/shibo/Workspace/cleanrl/videos/CartPole-v1__dqn__1__1772033784/rl-video-episode-2000.mp4\n",
      "global_step=54623, episodic_return=[13.]\n",
      "global_step=54692, episodic_return=[69.]\n",
      "SPS: 8515\n",
      "global_step=54707, episodic_return=[15.]\n",
      "global_step=54771, episodic_return=[64.]\n",
      "SPS: 8518\n",
      "global_step=54816, episodic_return=[45.]\n",
      "global_step=54847, episodic_return=[31.]\n",
      "global_step=54868, episodic_return=[21.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 8520\n",
      "global_step=54936, episodic_return=[68.]\n",
      "global_step=54959, episodic_return=[23.]\n",
      "SPS: 8524\n",
      "global_step=55011, episodic_return=[52.]\n",
      "global_step=55026, episodic_return=[15.]\n",
      "global_step=55038, episodic_return=[12.]\n",
      "SPS: 8526\n",
      "global_step=55186, episodic_return=[148.]\n",
      "global_step=55197, episodic_return=[11.]\n",
      "SPS: 8530\n",
      "global_step=55254, episodic_return=[57.]\n",
      "global_step=55268, episodic_return=[14.]\n",
      "SPS: 8533\n",
      "global_step=55315, episodic_return=[47.]\n",
      "global_step=55348, episodic_return=[33.]\n",
      "global_step=55396, episodic_return=[48.]\n",
      "SPS: 8536\n",
      "global_step=55425, episodic_return=[29.]\n",
      "global_step=55456, episodic_return=[31.]\n",
      "global_step=55485, episodic_return=[29.]\n",
      "SPS: 8539\n",
      "global_step=55531, episodic_return=[46.]\n",
      "global_step=55546, episodic_return=[15.]\n",
      "global_step=55561, episodic_return=[15.]\n",
      "SPS: 8542\n",
      "global_step=55609, episodic_return=[48.]\n",
      "global_step=55628, episodic_return=[19.]\n",
      "global_step=55648, episodic_return=[20.]\n",
      "global_step=55683, episodic_return=[35.]\n",
      "SPS: 8545\n",
      "global_step=55724, episodic_return=[41.]\n",
      "global_step=55762, episodic_return=[38.]\n",
      "SPS: 8548\n",
      "global_step=55837, episodic_return=[75.]\n",
      "global_step=55851, episodic_return=[14.]\n",
      "global_step=55892, episodic_return=[41.]\n",
      "SPS: 8550\n",
      "global_step=55944, episodic_return=[52.]\n",
      "SPS: 8555\n",
      "global_step=56011, episodic_return=[67.]\n",
      "global_step=56043, episodic_return=[32.]\n",
      "global_step=56069, episodic_return=[26.]\n",
      "global_step=56084, episodic_return=[15.]\n",
      "SPS: 8557\n",
      "global_step=56156, episodic_return=[72.]\n",
      "global_step=56168, episodic_return=[12.]\n",
      "global_step=56188, episodic_return=[20.]\n",
      "global_step=56200, episodic_return=[12.]\n",
      "SPS: 8560\n",
      "global_step=56216, episodic_return=[16.]\n",
      "global_step=56252, episodic_return=[36.]\n",
      "global_step=56277, episodic_return=[25.]\n",
      "SPS: 8562\n",
      "global_step=56319, episodic_return=[42.]\n",
      "global_step=56342, episodic_return=[23.]\n",
      "global_step=56373, episodic_return=[31.]\n",
      "SPS: 8561\n",
      "global_step=56408, episodic_return=[35.]\n",
      "global_step=56437, episodic_return=[29.]\n",
      "SPS: 8565\n",
      "global_step=56503, episodic_return=[66.]\n",
      "global_step=56524, episodic_return=[21.]\n",
      "global_step=56567, episodic_return=[43.]\n",
      "global_step=56577, episodic_return=[10.]\n",
      "global_step=56591, episodic_return=[14.]\n",
      "SPS: 8566\n",
      "global_step=56665, episodic_return=[74.]\n",
      "SPS: 8569\n",
      "global_step=56784, episodic_return=[119.]\n",
      "SPS: 8572\n",
      "global_step=56820, episodic_return=[36.]\n",
      "global_step=56856, episodic_return=[36.]\n",
      "global_step=56866, episodic_return=[10.]\n",
      "global_step=56893, episodic_return=[27.]\n",
      "SPS: 8573\n",
      "global_step=56908, episodic_return=[15.]\n",
      "global_step=56957, episodic_return=[49.]\n",
      "global_step=56975, episodic_return=[18.]\n",
      "SPS: 8574\n",
      "global_step=57009, episodic_return=[34.]\n",
      "global_step=57091, episodic_return=[82.]\n",
      "SPS: 8576\n",
      "global_step=57111, episodic_return=[20.]\n",
      "global_step=57135, episodic_return=[24.]\n",
      "global_step=57151, episodic_return=[16.]\n",
      "SPS: 8570\n",
      "global_step=57286, episodic_return=[135.]\n",
      "SPS: 8573\n",
      "global_step=57301, episodic_return=[15.]\n",
      "global_step=57340, episodic_return=[39.]\n",
      "global_step=57351, episodic_return=[11.]\n",
      "global_step=57365, episodic_return=[14.]\n",
      "global_step=57396, episodic_return=[31.]\n",
      "SPS: 8574\n",
      "global_step=57406, episodic_return=[10.]\n",
      "global_step=57450, episodic_return=[44.]\n",
      "global_step=57468, episodic_return=[18.]\n",
      "SPS: 8576\n",
      "global_step=57513, episodic_return=[45.]\n",
      "global_step=57541, episodic_return=[28.]\n",
      "global_step=57576, episodic_return=[35.]\n",
      "global_step=57598, episodic_return=[22.]\n",
      "SPS: 8578\n",
      "global_step=57624, episodic_return=[26.]\n",
      "global_step=57642, episodic_return=[18.]\n",
      "global_step=57663, episodic_return=[21.]\n",
      "SPS: 8580\n",
      "global_step=57738, episodic_return=[75.]\n",
      "SPS: 8585\n",
      "global_step=57833, episodic_return=[95.]\n",
      "global_step=57848, episodic_return=[15.]\n",
      "global_step=57900, episodic_return=[52.]\n",
      "SPS: 8589\n",
      "global_step=57923, episodic_return=[23.]\n",
      "global_step=57985, episodic_return=[62.]\n",
      "SPS: 8592\n",
      "global_step=58014, episodic_return=[29.]\n",
      "global_step=58073, episodic_return=[59.]\n",
      "SPS: 8595\n",
      "global_step=58102, episodic_return=[29.]\n",
      "global_step=58170, episodic_return=[68.]\n",
      "global_step=58198, episodic_return=[28.]\n",
      "SPS: 8598\n",
      "global_step=58281, episodic_return=[83.]\n",
      "SPS: 8602\n",
      "global_step=58349, episodic_return=[68.]\n",
      "global_step=58367, episodic_return=[18.]\n",
      "global_step=58393, episodic_return=[26.]\n",
      "SPS: 8604\n",
      "global_step=58407, episodic_return=[14.]\n",
      "global_step=58444, episodic_return=[37.]\n",
      "global_step=58460, episodic_return=[16.]\n",
      "SPS: 8606\n",
      "global_step=58511, episodic_return=[51.]\n",
      "global_step=58594, episodic_return=[83.]\n",
      "SPS: 8610\n",
      "global_step=58617, episodic_return=[23.]\n",
      "global_step=58637, episodic_return=[20.]\n",
      "global_step=58682, episodic_return=[45.]\n",
      "SPS: 8612\n",
      "global_step=58765, episodic_return=[83.]\n",
      "SPS: 8613\n",
      "global_step=58802, episodic_return=[37.]\n",
      "global_step=58837, episodic_return=[35.]\n",
      "global_step=58892, episodic_return=[55.]\n",
      "SPS: 8613\n",
      "global_step=58910, episodic_return=[18.]\n",
      "global_step=58946, episodic_return=[36.]\n",
      "global_step=58957, episodic_return=[11.]\n",
      "global_step=58979, episodic_return=[22.]\n",
      "SPS: 8615\n",
      "global_step=59007, episodic_return=[28.]\n",
      "global_step=59019, episodic_return=[12.]\n",
      "SPS: 8617\n",
      "global_step=59104, episodic_return=[85.]\n",
      "SPS: 8620\n",
      "global_step=59236, episodic_return=[132.]\n",
      "global_step=59265, episodic_return=[29.]\n",
      "global_step=59280, episodic_return=[15.]\n",
      "SPS: 8623\n",
      "global_step=59306, episodic_return=[26.]\n",
      "global_step=59321, episodic_return=[15.]\n",
      "global_step=59374, episodic_return=[53.]\n",
      "SPS: 8624\n",
      "global_step=59450, episodic_return=[76.]\n",
      "global_step=59479, episodic_return=[29.]\n",
      "SPS: 8628\n",
      "global_step=59598, episodic_return=[119.]\n",
      "SPS: 8631\n",
      "global_step=59644, episodic_return=[46.]\n",
      "SPS: 8634\n",
      "global_step=59727, episodic_return=[83.]\n",
      "global_step=59758, episodic_return=[31.]\n",
      "global_step=59796, episodic_return=[38.]\n",
      "SPS: 8637\n",
      "global_step=59852, episodic_return=[56.]\n",
      "SPS: 8640\n",
      "global_step=59903, episodic_return=[51.]\n",
      "global_step=59919, episodic_return=[16.]\n",
      "global_step=59940, episodic_return=[21.]\n",
      "SPS: 8643\n",
      "global_step=60006, episodic_return=[66.]\n",
      "global_step=60028, episodic_return=[22.]\n",
      "global_step=60042, episodic_return=[14.]\n",
      "global_step=60085, episodic_return=[43.]\n",
      "SPS: 8643\n",
      "global_step=60153, episodic_return=[68.]\n",
      "global_step=60179, episodic_return=[26.]\n",
      "SPS: 8646\n",
      "global_step=60294, episodic_return=[115.]\n",
      "SPS: 8650\n",
      "global_step=60355, episodic_return=[61.]\n",
      "global_step=60398, episodic_return=[43.]\n",
      "SPS: 8653\n",
      "global_step=60430, episodic_return=[32.]\n",
      "global_step=60446, episodic_return=[16.]\n",
      "global_step=60458, episodic_return=[12.]\n",
      "global_step=60494, episodic_return=[36.]\n",
      "SPS: 8656\n",
      "global_step=60586, episodic_return=[92.]\n",
      "SPS: 8660\n",
      "global_step=60651, episodic_return=[65.]\n",
      "global_step=60699, episodic_return=[48.]\n",
      "SPS: 8664\n",
      "global_step=60711, episodic_return=[12.]\n",
      "global_step=60745, episodic_return=[34.]\n",
      "global_step=60768, episodic_return=[23.]\n",
      "global_step=60788, episodic_return=[20.]\n",
      "SPS: 8664\n",
      "global_step=60803, episodic_return=[15.]\n",
      "global_step=60833, episodic_return=[30.]\n",
      "SPS: 8666\n",
      "global_step=60931, episodic_return=[98.]\n",
      "SPS: 8670\n",
      "global_step=61016, episodic_return=[85.]\n",
      "global_step=61036, episodic_return=[20.]\n",
      "global_step=61086, episodic_return=[50.]\n",
      "SPS: 8673\n",
      "global_step=61105, episodic_return=[19.]\n",
      "global_step=61154, episodic_return=[49.]\n",
      "global_step=61178, episodic_return=[24.]\n",
      "SPS: 8671\n",
      "global_step=61213, episodic_return=[35.]\n",
      "global_step=61255, episodic_return=[42.]\n",
      "global_step=61264, episodic_return=[9.]\n",
      "SPS: 8673\n",
      "global_step=61301, episodic_return=[37.]\n",
      "SPS: 8675\n",
      "global_step=61411, episodic_return=[110.]\n",
      "global_step=61475, episodic_return=[64.]\n",
      "SPS: 8675\n",
      "global_step=61567, episodic_return=[92.]\n",
      "global_step=61584, episodic_return=[17.]\n",
      "SPS: 8678\n",
      "global_step=61614, episodic_return=[30.]\n",
      "global_step=61636, episodic_return=[22.]\n",
      "global_step=61660, episodic_return=[24.]\n",
      "global_step=61681, episodic_return=[21.]\n",
      "SPS: 8680\n",
      "global_step=61716, episodic_return=[35.]\n",
      "global_step=61741, episodic_return=[25.]\n",
      "global_step=61756, episodic_return=[15.]\n",
      "global_step=61786, episodic_return=[30.]\n",
      "SPS: 8682\n",
      "global_step=61804, episodic_return=[18.]\n",
      "global_step=61858, episodic_return=[54.]\n",
      "SPS: 8685\n",
      "global_step=61949, episodic_return=[91.]\n",
      "global_step=61969, episodic_return=[20.]\n",
      "SPS: 8689\n",
      "SPS: 8693\n",
      "global_step=62137, episodic_return=[168.]\n",
      "global_step=62165, episodic_return=[28.]\n",
      "global_step=62183, episodic_return=[18.]\n",
      "SPS: 8696\n",
      "global_step=62298, episodic_return=[115.]\n",
      "SPS: 8700\n",
      "global_step=62330, episodic_return=[32.]\n",
      "global_step=62343, episodic_return=[13.]\n",
      "global_step=62376, episodic_return=[33.]\n",
      "global_step=62387, episodic_return=[11.]\n",
      "SPS: 8703\n",
      "global_step=62426, episodic_return=[39.]\n",
      "global_step=62451, episodic_return=[25.]\n",
      "SPS: 8705\n",
      "global_step=62512, episodic_return=[61.]\n",
      "global_step=62531, episodic_return=[19.]\n",
      "global_step=62555, episodic_return=[24.]\n",
      "global_step=62568, episodic_return=[13.]\n",
      "global_step=62599, episodic_return=[31.]\n",
      "SPS: 8706\n",
      "SPS: 8710\n",
      "global_step=62737, episodic_return=[138.]\n",
      "global_step=62757, episodic_return=[20.]\n",
      "SPS: 8713\n",
      "global_step=62801, episodic_return=[44.]\n",
      "global_step=62840, episodic_return=[39.]\n",
      "global_step=62891, episodic_return=[51.]\n",
      "SPS: 8714\n",
      "global_step=62939, episodic_return=[48.]\n",
      "global_step=62976, episodic_return=[37.]\n",
      "SPS: 8717\n",
      "global_step=63015, episodic_return=[39.]\n",
      "global_step=63091, episodic_return=[76.]\n",
      "SPS: 8719\n",
      "global_step=63106, episodic_return=[15.]\n",
      "global_step=63137, episodic_return=[31.]\n",
      "SPS: 8719\n",
      "global_step=63213, episodic_return=[76.]\n",
      "global_step=63233, episodic_return=[20.]\n",
      "global_step=63263, episodic_return=[30.]\n",
      "SPS: 8719\n",
      "global_step=63309, episodic_return=[46.]\n",
      "global_step=63351, episodic_return=[42.]\n",
      "SPS: 8722\n",
      "global_step=63413, episodic_return=[62.]\n",
      "global_step=63426, episodic_return=[13.]\n",
      "global_step=63451, episodic_return=[25.]\n",
      "global_step=63494, episodic_return=[43.]\n",
      "SPS: 8724\n",
      "global_step=63523, episodic_return=[29.]\n",
      "global_step=63580, episodic_return=[57.]\n",
      "SPS: 8727\n",
      "global_step=63601, episodic_return=[21.]\n",
      "global_step=63610, episodic_return=[9.]\n",
      "global_step=63637, episodic_return=[27.]\n",
      "SPS: 8730\n",
      "global_step=63752, episodic_return=[115.]\n",
      "global_step=63777, episodic_return=[25.]\n",
      "global_step=63790, episodic_return=[13.]\n",
      "SPS: 8734\n",
      "global_step=63814, episodic_return=[24.]\n",
      "global_step=63867, episodic_return=[53.]\n",
      "global_step=63897, episodic_return=[30.]\n",
      "SPS: 8736\n",
      "global_step=63940, episodic_return=[43.]\n",
      "global_step=63977, episodic_return=[37.]\n",
      "SPS: 8737\n",
      "global_step=64006, episodic_return=[29.]\n",
      "global_step=64020, episodic_return=[14.]\n",
      "global_step=64036, episodic_return=[16.]\n",
      "global_step=64099, episodic_return=[63.]\n",
      "SPS: 8736\n",
      "global_step=64147, episodic_return=[48.]\n",
      "SPS: 8737\n",
      "global_step=64249, episodic_return=[102.]\n",
      "SPS: 8739\n",
      "global_step=64306, episodic_return=[57.]\n",
      "global_step=64326, episodic_return=[20.]\n",
      "global_step=64359, episodic_return=[33.]\n",
      "SPS: 8741\n",
      "global_step=64465, episodic_return=[106.]\n",
      "global_step=64490, episodic_return=[25.]\n",
      "SPS: 8744\n",
      "global_step=64552, episodic_return=[62.]\n",
      "global_step=64579, episodic_return=[27.]\n",
      "global_step=64593, episodic_return=[14.]\n",
      "SPS: 8745\n",
      "global_step=64606, episodic_return=[13.]\n",
      "global_step=64639, episodic_return=[33.]\n",
      "global_step=64653, episodic_return=[14.]\n",
      "global_step=64668, episodic_return=[15.]\n",
      "global_step=64685, episodic_return=[17.]\n",
      "SPS: 8746\n",
      "global_step=64746, episodic_return=[61.]\n",
      "global_step=64772, episodic_return=[26.]\n",
      "SPS: 8749\n",
      "global_step=64806, episodic_return=[34.]\n",
      "SPS: 8751\n",
      "global_step=64911, episodic_return=[105.]\n",
      "global_step=64923, episodic_return=[12.]\n",
      "SPS: 8753\n",
      "global_step=65002, episodic_return=[79.]\n",
      "global_step=65013, episodic_return=[11.]\n",
      "global_step=65093, episodic_return=[80.]\n",
      "SPS: 8753\n",
      "global_step=65133, episodic_return=[40.]\n",
      "global_step=65162, episodic_return=[29.]\n",
      "global_step=65173, episodic_return=[11.]\n",
      "global_step=65189, episodic_return=[16.]\n",
      "SPS: 8753\n",
      "global_step=65249, episodic_return=[60.]\n",
      "SPS: 8755\n",
      "global_step=65327, episodic_return=[78.]\n"
     ]
    }
   ],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/dqn/#dqnpy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from cleanrl_utils.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    # exp_name: str = os.path.basename(__file__)[: -len(\".py\")]\n",
    "    exp_name: str = \"dqn\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    buffer_size: int = 10000\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 1.0\n",
    "    \"\"\"the target network update rate\"\"\"\n",
    "    target_network_frequency: int = 500\n",
    "    \"\"\"the timesteps it takes to update the target network\"\"\"\n",
    "    batch_size: int = 128\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    start_e: float = 1\n",
    "    \"\"\"the starting epsilon for exploration\"\"\"\n",
    "    end_e: float = 0.05\n",
    "    \"\"\"the ending epsilon for exploration\"\"\"\n",
    "    exploration_fraction: float = 0.5\n",
    "    \"\"\"the fraction of `total-timesteps` it takes from start-e to go end-e\"\"\"\n",
    "    learning_starts: int = 10000\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    train_frequency: int = 10\n",
    "    \"\"\"the frequency of training\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = tyro.cli(Args)\n",
    "    args = tyro.cli(Args, args=[\"--env-id\", \"CartPole-v1\", \"--capture_video\"])\n",
    "    assert args.num_envs == 1, \"vectorized envs are not supported at the moment\"\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "    q_network = QNetwork(envs).to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
    "    target_network = QNetwork(envs).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        envs.single_observation_space,\n",
    "        envs.single_action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    obs, _ = envs.reset(seed=args.seed)\n",
    "    for global_step in range(args.total_timesteps):\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
    "        if random.random() < epsilon:\n",
    "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "        else:\n",
    "            q_values = q_network(torch.Tensor(obs).to(device))\n",
    "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for idx, trunc in enumerate(truncations):\n",
    "            if trunc:\n",
    "                real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        obs = next_obs\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > args.learning_starts:\n",
    "            if global_step % args.train_frequency == 0:\n",
    "                data = rb.sample(args.batch_size)\n",
    "                with torch.no_grad():\n",
    "                    target_max, _ = target_network(data.next_observations).max(dim=1)\n",
    "                    td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
    "                old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "                loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "                if global_step % 100 == 0:\n",
    "                    writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
    "                    writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
    "                    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "                # optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # update target network\n",
    "            if global_step % args.target_network_frequency == 0:\n",
    "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
    "                    target_network_param.data.copy_(\n",
    "                        args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
    "                    )\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "        torch.save(q_network.state_dict(), model_path)\n",
    "        print(f\"model saved to {model_path}\")\n",
    "        from cleanrl_utils.evals.dqn_eval import evaluate\n",
    "\n",
    "        episodic_returns = evaluate(\n",
    "            model_path,\n",
    "            make_env,\n",
    "            args.env_id,\n",
    "            eval_episodes=10,\n",
    "            run_name=f\"{run_name}-eval\",\n",
    "            Model=QNetwork,\n",
    "            device=device,\n",
    "            epsilon=args.end_e,\n",
    "        )\n",
    "        for idx, episodic_return in enumerate(episodic_returns):\n",
    "            writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "        if args.upload_model:\n",
    "            from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "            repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "            repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "            push_to_hub(args, episodic_returns, repo_id, \"DQN\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7019837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
